{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微信恋爱报告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "df = pd.read_csv(\"./妠妠老婆🍪.csv\")\n",
    "df = df[[\"StrContent\", \"StrTime\", \"Remark\"]]\n",
    "df = df.astype(str)\n",
    "os.makedirs(\"./mzz_data/\", exist_ok=True)\n",
    "\n",
    "# 将这里替换为你们的微信昵称\n",
    "remark_1 = \"小老正\"\n",
    "remark_2 = \"妠妠老婆🍪\"\n",
    "\n",
    "# 这里替换为你们的姓名简写\n",
    "name_1 = \"mzz\"\n",
    "name_2 = \"syn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 发送表情包的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_emoji\"] = df[\"StrContent\"].apply(lambda x: \"emoji\" in x)\n",
    "# 总的表情包发送次数\n",
    "print(\"总的表情包发送次数：\" + str(df.loc[df[\"is_emoji\"]==True, \"StrContent\"].count()))\n",
    "# 小老正的表情包发送次数\n",
    "print(f\"{remark_1}的表情包发送次数：\" + str(df.loc[(df[\"is_emoji\"]==True) & (df[\"Remark\"]==remark_1), \"StrContent\"].count()))\n",
    "# 妠妠老婆🍪的表情包发送次数\n",
    "print(f\"{remark_2}的表情包发送次数：\" + str(df.loc[(df[\"is_emoji\"]==True) & (df[\"Remark\"]==remark_2), \"StrContent\"].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "# 统计表情包发送次数\n",
    "total_emoji = df.loc[df[\"is_emoji\"] == True, \"StrContent\"].count()\n",
    "mzz_emoji = df.loc[(df[\"is_emoji\"] == True) & (df[\"Remark\"] == remark_1), \"StrContent\"].count()\n",
    "syn_emoji = df.loc[(df[\"is_emoji\"] == True) & (df[\"Remark\"] == remark_2), \"StrContent\"].count()\n",
    "# 数据准备\n",
    "labels = [name_1, name_2]  # 使用简写\n",
    "sizes = [mzz_emoji, syn_emoji]  # 各部分的大小\n",
    "colors = [\"#FFB6C1\", \"#FFF5E1\"]  # 粉色和奶油色\n",
    "explode = (0, 0.1)  # 突出显示前两部分\n",
    "\n",
    "# 启用手绘风格\n",
    "plt.xkcd()\n",
    "\n",
    "# 绘制饼图\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct=\"%1.1f%%\", startangle=90, shadow=True)\n",
    "\n",
    "# 设置英文标题\n",
    "plt.title(\"Pie of Emoji\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# 保存\n",
    "plt.savefig(\"./mzz_data/表情包发送次数饼图.png\", dpi=300)\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最常发送的表情包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# 下载表情包图片的函数\n",
    "def download_emoji_image(emoji_xml, dir_path, rank, count):\n",
    "    # 解析XML\n",
    "    root = ET.fromstring(emoji_xml)\n",
    "\n",
    "    # 找到emoji标签\n",
    "    emoji_element = root.find('.//emoji')\n",
    "    if emoji_element is None:\n",
    "        raise ValueError(\"未找到emoji标签\")\n",
    "\n",
    "    # 获取cdnurl\n",
    "    cdnurl = emoji_element.get('cdnurl')\n",
    "    if not cdnurl:\n",
    "        raise ValueError(\"cdnurl未找到\")\n",
    "\n",
    "    # 下载图片\n",
    "    response = requests.get(cdnurl)\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"下载失败，状态码: {response.status_code}\")\n",
    "\n",
    "    # 保存图片，命名为“排名_发送次数.jpg”\n",
    "    filename = os.path.join(dir_path, f\"{rank}_{count}.jpg\")\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f\"图片已下载并保存为 {filename}\")\n",
    "\n",
    "# 创建目录\n",
    "dir_path = f\"./mzz_data/emoji/{remark_2}最常发的图片\"\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# 获取最常发的表情包XML列表及其发送次数\n",
    "emoji_stats = df.loc[(df[\"is_emoji\"] == True) & (df[\"Remark\"] == remark_2), \"StrContent\"].value_counts().head(10)\n",
    "\n",
    "# 遍历每个表情包XML，下载并命名\n",
    "for rank, (emoji_xml, count) in enumerate(emoji_stats.items()):\n",
    "    download_emoji_image(emoji_xml, dir_path, rank, count)\n",
    "\n",
    "# 创建目录\n",
    "dir_path = f\"./mzz_data/emoji/{remark_1}最常发的图片\"\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# 获取最常发的表情包XML列表及其发送次数\n",
    "emoji_stats = df.loc[(df[\"is_emoji\"] == True) & (df[\"Remark\"] == remark_1), \"StrContent\"].value_counts().head(10)\n",
    "\n",
    "# 遍历每个表情包XML，下载并命名\n",
    "for rank, (emoji_xml, count) in enumerate(emoji_stats.items()):\n",
    "    download_emoji_image(emoji_xml, dir_path, rank, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "\n",
    "# 启用 xkcd 模式\n",
    "plt.xkcd()\n",
    "\n",
    "# 设置字体为黑体，解决中文显示问题\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # 解决负号显示为方块的问题\n",
    "\n",
    "\n",
    "# 将 StrTime 转换为 datetime 类型\n",
    "df[\"StrTime\"] = pd.to_datetime(df[\"StrTime\"])\n",
    "\n",
    "# 提取小时信息\n",
    "df[\"Hour\"] = df[\"StrTime\"].dt.hour\n",
    "\n",
    "import re\n",
    "\n",
    "# 定义函数，统计汉字长度\n",
    "def count_chinese_characters(text):\n",
    "    # 匹配所有中文字符（包括简体和繁体）\n",
    "    chinese_characters = re.findall(r'[\\u4e00-\\u9fff]', text)\n",
    "    return len(chinese_characters)\n",
    "\n",
    "# 应用到 DataFrame 中\n",
    "df[\"WordCount\"] = df[\"StrContent\"].apply(count_chinese_characters)\n",
    "\n",
    "# 按小时统计聊天字数\n",
    "hourly_word_count = df.groupby(\"Hour\")[\"WordCount\"].sum()\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 使用 plt.bar 绘制柱状图\n",
    "plt.bar(hourly_word_count.index, hourly_word_count.values, color=\"mediumseagreen\", edgecolor=\"black\", linewidth=2, alpha=0.7)\n",
    "\n",
    "# 设置标题和标签\n",
    "plt.title(\"Word Count Per Hour\", fontsize=16)\n",
    "plt.xlabel(\"Hour\", fontsize=12)\n",
    "plt.ylabel(\"Word Count\", fontsize=12)\n",
    "\n",
    "# 设置 x 轴刻度\n",
    "plt.xticks(range(24), [f\"{i}:00\" for i in range(24)], rotation=45)\n",
    "\n",
    "# 调整布局\n",
    "plt.tight_layout()\n",
    "\n",
    "# 保存\n",
    "plt.savefig(\"./mzz_data/hourly_word_count.png\")\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 聊天日期Github热力图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import calendar\n",
    "\n",
    "# 将 StrTime 转换为 datetime 类型\n",
    "df[\"StrTime\"] = pd.to_datetime(df[\"StrTime\"])\n",
    "\n",
    "# 提取日期\n",
    "df[\"Date\"] = df[\"StrTime\"].dt.date\n",
    "\n",
    "# 确保 Date 列为 datetime 类型\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# 过滤2024年的数据\n",
    "df_2024 = df[df['Date'].dt.year == 2024]\n",
    "\n",
    "# 按日期统计聊天字数\n",
    "daily_word_count = df_2024.groupby('Date')['WordCount'].sum().reset_index()\n",
    "\n",
    "# 使用箱线图去掉异常值\n",
    "Q1 = daily_word_count['WordCount'].quantile(0.25)\n",
    "Q3 = daily_word_count['WordCount'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# 异常值填补为边界\n",
    "daily_word_count.loc[daily_word_count['WordCount'] > upper_bound, 'WordCount'] = upper_bound\n",
    "\n",
    "# 为热力图创建矩阵\n",
    "daily_word_count['Weekday'] = daily_word_count['Date'].dt.weekday  # 星期几\n",
    "daily_word_count['Week'] = daily_word_count['Date'].dt.strftime('%U')  # 第几周\n",
    "\n",
    "# 创建一个全年的空矩阵 (52周 x 7天)\n",
    "heatmap_data = pd.DataFrame(\n",
    "    index=range(7), columns=range(1, 53), data=0\n",
    ")\n",
    "\n",
    "# 填充矩阵\n",
    "for _, row in daily_word_count.iterrows():\n",
    "    week = int(row['Week'])\n",
    "    weekday = row['Weekday']\n",
    "    heatmap_data.loc[weekday, week] = row['WordCount']\n",
    "\n",
    "\n",
    "# 获取每个月的起始日期对应的周数\n",
    "first_day_of_month = pd.date_range(start='2024-01-01', end='2024-12-31', freq='MS')\n",
    "month_labels = {int(date.strftime('%U')): date.strftime('%b') for date in first_day_of_month}\n",
    "\n",
    "# 修改绘图代码\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    cmap='Greens',\n",
    "    linewidths=0.5,\n",
    "    linecolor='white',\n",
    "    cbar_kws={'label': 'Word Count'},\n",
    "    square=True\n",
    ")\n",
    "\n",
    "\n",
    "# 替换横坐标的刻度标签\n",
    "weeks = range(0, 53)\n",
    "xtick_labels = [month_labels[week] if week in month_labels else '' for week in weeks]\n",
    "plt.xticks(ticks=range(0, 53), labels=xtick_labels, rotation=0)\n",
    "# 设置标题和其他样式\n",
    "plt.title('Chat Activity Heatmap', fontsize=16)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Weekday')\n",
    "plt.yticks(ticks=range(7), labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'], rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./mzz_data/heatmap.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聊天字数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总聊天次数\n",
    "print(\"总聊天次数：\" + str(df[\"WordCount\"].sum()))\n",
    "# 小老正的聊天字数\n",
    "print(f\"{remark_1}的聊天字数：\" + str(df.loc[df[\"Remark\"]==remark_1, \"WordCount\"].sum()))\n",
    "# 妠妠老婆🍪的聊天字数\n",
    "print(f\"{remark_2}的聊天字数：\" + str(df.loc[df[\"Remark\"]==remark_2, \"WordCount\"].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "# 统计表情包发送次数\n",
    "total_word = df[\"WordCount\"].sum()\n",
    "mzz_word = df.loc[df[\"Remark\"]==remark_1, \"WordCount\"].sum()\n",
    "syn_word = df.loc[df[\"Remark\"]==remark_2, \"WordCount\"].sum()\n",
    "# 数据准备\n",
    "labels = [name_1, name_2]  # 使用简写\n",
    "sizes = [mzz_word, syn_word]  # 各部分的大小\n",
    "colors = [\"#FFB6C1\", \"#FFF5E1\"]  # 粉色和奶油色\n",
    "explode = (0, 0.1)  # 突出显示前两部分\n",
    "\n",
    "# 启用手绘风格\n",
    "plt.xkcd()\n",
    "\n",
    "# 绘制饼图\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct=\"%1.1f%%\", startangle=90, shadow=True)\n",
    "\n",
    "# 设置英文标题\n",
    "plt.title(\"Pie of word\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# 保存\n",
    "plt.savefig(\"./mzz_data/字数次数饼图.png\", dpi=300)\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聊天时间段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 将 StrTime 转换为 datetime 类型\n",
    "df[\"StrTime\"] = pd.to_datetime(df[\"StrTime\"])\n",
    "\n",
    "# 提取日期\n",
    "df[\"Date\"] = df[\"StrTime\"].dt.date\n",
    "\n",
    "# 统计每日消息数量\n",
    "daily_message_count = df.loc[df[\"is_emoji\"]==False].groupby(\"Date\").size()\n",
    "\n",
    "# 看看聊天最频繁是哪天\n",
    "print(\"聊天最频繁是哪天：\" + str(daily_message_count.index[daily_message_count.argsort()[::-1][1]]))\n",
    "\n",
    "# 看看这天说了什么\n",
    "text = \"\\n\".join(df.loc[df[\"Date\"]==daily_message_count.index[daily_message_count.argsort()[::-1][1]]][\"StrContent\"].values)\n",
    "# 正则表达式匹配中文字符和标点符号，同时保留换行符\n",
    "pattern = re.compile(r'[^\\u4e00-\\u9fff\\u3000-\\u303f\\uff00-\\uffef\\n:]')\n",
    "\n",
    "# 替换掉非中文字符、标点符号和换行符\n",
    "filtered_text = re.sub(pattern, '', text)\n",
    "\n",
    "# 去除多余的空行（保留单行换行）\n",
    "filtered_text = re.sub(r'\\n+', '\\n', filtered_text).strip()\n",
    "\n",
    "# 这天说了多少字\n",
    "print(\"这天说了多少字：\" + str(len(filtered_text)))\n",
    "# 这天说了什么\n",
    "print(\"这天说了什么：\\n\" + filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聊天字数随时间变化折线图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# 将 StrTime 转换为 datetime 类型\n",
    "df['StrTime'] = pd.to_datetime(df['StrTime'])\n",
    "\n",
    "# 过滤出你和女朋友的聊天记录\n",
    "syn_df = df[df['Remark'] == remark_2]  # 女朋友的聊天记录\n",
    "mzz_df = df[df['Remark'] == remark_1]      # 你的聊天记录\n",
    "\n",
    "# 按月份统计字数\n",
    "syn_df['Month'] = syn_df['StrTime'].dt.to_period('M')\n",
    "mzz_df['Month'] = mzz_df['StrTime'].dt.to_period('M')\n",
    "\n",
    "syn_monthly_word_count = syn_df.groupby('Month')['WordCount'].sum().iloc[: -1]\n",
    "mzz_monthly_word_count = mzz_df.groupby('Month')['WordCount'].sum().iloc[: -1]\n",
    "\n",
    "# 找到最高点的月份和字数\n",
    "syn_max_month = syn_monthly_word_count.index.astype(str)[syn_monthly_word_count.argmax()]\n",
    "syn_max_word_count = syn_monthly_word_count.max()\n",
    "\n",
    "mzz_max_month = mzz_monthly_word_count.index.astype(str)[mzz_monthly_word_count.argmax()]\n",
    "mzz_max_word_count = mzz_monthly_word_count.max()\n",
    "\n",
    "# 启用 xkcd 风格\n",
    "plt.xkcd()\n",
    "\n",
    "# 绘制折线图\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# 绘制女朋友的折线图\n",
    "ax.plot(syn_monthly_word_count.index.astype(str), syn_monthly_word_count.values, \n",
    "        marker='o', linestyle='-', color='lightcoral', linewidth=2, markersize=8, label=f'{name_2.upper()}\\'s Word Count')\n",
    "\n",
    "# 绘制你的折线图\n",
    "ax.plot(mzz_monthly_word_count.index.astype(str), mzz_monthly_word_count.values, \n",
    "        marker='o', linestyle='-', color='mediumseagreen', linewidth=2, markersize=8, label=f'{name_1.upper()}\\'s Word Count')\n",
    "\n",
    "# 标记女朋友的最高点\n",
    "ax.plot(syn_max_month, syn_max_word_count, marker='*', markersize=20, color='coral', \n",
    "        markeredgecolor='black', markeredgewidth=1, label=f'{name_2.upper()}\\'s Highest Point')\n",
    "\n",
    "# 标记你的最高点\n",
    "ax.plot(mzz_max_month, mzz_max_word_count, marker='*', markersize=20, color='gold', \n",
    "        markeredgecolor='black', markeredgewidth=1, label=f'{name_1.upper()}\\'s Highest Point')\n",
    "\n",
    "# 设置标题和标签\n",
    "ax.set_title('Word Count Over Time', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Month', fontsize=12)\n",
    "ax.set_ylabel('Word Count', fontsize=12)\n",
    "\n",
    "# 旋转 x 轴标签\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 显示网格\n",
    "ax.grid(True, linestyle='-', alpha=0.6)\n",
    "\n",
    "# 显示图例\n",
    "ax.legend()\n",
    "\n",
    "# 自动调整布局\n",
    "plt.tight_layout()\n",
    "\n",
    "# 保存\n",
    "plt.savefig('./mzz_data/combined_word_count_over_time.png', dpi=300)\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2024年5月，我们进入热恋期，聊天字数达到了最高点（一些照片）\n",
    "2024年7月，老公放暑假啦，我们短暂的结束了异地时光，经常出去玩，聊天字数有所下降\n",
    "之后，我们的聊天字数持续上升，说明我们感情越来越好！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对话情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "# 初始化 OpenAI 客户端\n",
    "client = OpenAI(\n",
    "    api_key=\"XXX\",  # 替换为你的 API Key\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    ")\n",
    "\n",
    "\n",
    "# 将 StrTime 转换为 datetime 类型\n",
    "df['StrTime'] = pd.to_datetime(df['StrTime'])\n",
    "\n",
    "# 按天分组，拼接对话内容\n",
    "daily_conversations = df.groupby('Date').apply(lambda x: ' '.join(x['StrContent'])).reset_index(name='Conversation')\n",
    "\n",
    "# 定义 system_prompt，引导模型输出 JSON 格式的情感分析结果\n",
    "system_prompt = \"\"\"\n",
    "你是一个情感分析助手。用户会提供一段对话内容，请你分析这段对话的情感倾向，并输出 JSON 格式的结果。\n",
    "\n",
    "输出格式要求：\n",
    "{\n",
    "    \"sentiment\": \"甜蜜\" | \"吵架\" | \"平淡\"\n",
    "}\n",
    "\n",
    "分析规则：\n",
    "1. 如果对话中包含大量积极、温馨、甜蜜的内容（例如“爱你”、“想你”、“开心”等），则输出“甜蜜”。\n",
    "2. 如果对话中包含大量负面、争吵、不满的内容（例如“生气”、“讨厌”、“烦”等），则输出“吵架”。\n",
    "3. 如果对话内容主要是日常交流，没有明显的情感倾向，则输出“平淡”。\n",
    "\n",
    "请根据对话内容，严格按照上述规则输出 JSON 格式的结果。\n",
    "\"\"\"\n",
    "\n",
    "# 定义重试机制\n",
    "def analyze_sentiment_with_retry(conversation, max_retries=3, delay=2):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"deepseek-chat\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": conversation}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            return json.loads(response.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            time.sleep(delay)\n",
    "    return {\"sentiment\": \"平淡\"}  # 如果重试多次失败，默认返回“平淡”\n",
    "\n",
    "# 分析每一天的情感倾向\n",
    "sentiment_counts = {\"甜蜜\": 0, \"吵架\": 0, \"平淡\": 0}\n",
    "\n",
    "for index, row in tqdm(daily_conversations.iterrows(), total=daily_conversations.shape[0]):\n",
    "    date = row['Date']\n",
    "    conversation = row['Conversation']\n",
    "    \n",
    "    # 调用 API 分析情感\n",
    "    result = analyze_sentiment_with_retry(conversation)\n",
    "    sentiment = result.get(\"sentiment\", \"平淡\")  # 如果返回结果中没有 sentiment，默认设为“平淡”\n",
    "    \n",
    "    # 统计情感天数\n",
    "    sentiment_counts[sentiment] += 1\n",
    "    \n",
    "    # 打印结果\n",
    "    print(f\"日期: {date}, 情感: {sentiment}\")\n",
    "\n",
    "# 打印最终统计结果\n",
    "print(\"\\n情感统计结果:\")\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    print(f\"{sentiment}: {count} 天\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 标签和数据\n",
    "labels = sentiment_counts.keys()\n",
    "sizes = sentiment_counts.values()\n",
    "\n",
    "# 颜色设置（草莓蛋糕风格）\n",
    "colors = [\"#FFB6C1\", \"#A52A2A\", \"#FFF5E1\"]  # 粉色、深红色、奶油色\n",
    "explode = (0.1, 0, 0)  # 突出显示“Sweet”部分\n",
    "\n",
    "# 启用手绘风格\n",
    "plt.xkcd()\n",
    "# 自定义显示函数\n",
    "def autopct_format(values):\n",
    "    def my_format(pct):\n",
    "        total = sum(values)\n",
    "        val = int(round(pct * total / 100.0))\n",
    "        return f'{val}'  # 显示原始数字\n",
    "    return my_format\n",
    "\n",
    "# 绘制饼图\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct=autopct_format(sizes), startangle=90, shadow=True, wedgeprops={'edgecolor': 'black'})\n",
    "\n",
    "# 设置英文标题\n",
    "plt.title(\"Relationship Day Statistics\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# 保存\n",
    "plt.savefig(\"./mzz_data/relationship_day_statistics.png\", dpi=300)\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "import re \n",
    "\n",
    "\n",
    "def clearn_text(text):\n",
    "    # 正则表达式匹配中文字符和标点符号，同时保留换行符\n",
    "    pattern = re.compile(r'[^\\u4e00-\\u9fff\\u3000-\\u303f\\uff00-\\uffef\\n]')\n",
    "\n",
    "    # 替换掉非中文字符、标点符号和换行符\n",
    "    filtered_text = re.sub(pattern, '', text)\n",
    "\n",
    "    # 去除多余的空行（保留单行换行）\n",
    "    filtered_text = re.sub(r'\\n+', '\\n', filtered_text).strip()\n",
    "    return filtered_text\n",
    "# 初始化 OpenAI 客户端\n",
    "client = OpenAI(\n",
    "    api_key=\"XXX\",  # 替换为你的 API Key\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    ")\n",
    "\n",
    "\n",
    "# 将 StrTime 转换为 datetime 类型\n",
    "df['StrTime'] = pd.to_datetime(df['StrTime'])\n",
    "\n",
    "template = \"\"\"我正在制作我与女朋友的恋爱报告，我将给你我们某一天的对话内容，这一天的内容若含有非常甜蜜非常感人的句子，就将其输出为列表，若这一天的内容很平淡，就输出空列表。\n",
    "以下是对话内容：\n",
    "```conversation\n",
    "{conversation}\n",
    "```\n",
    "最终请输出为json格式，格式示例为：\n",
    "{{\n",
    "    \"老公\": [\"甜蜜的句子1\", \"甜蜜的句子2\", ...],\n",
    "    \"老婆\": [\"甜蜜的句子1\", \"甜蜜的句子2\", ...]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "results = []\n",
    "for date, df_1 in tqdm(df.groupby([\"Date\"]), total=df.groupby([\"Date\"]).ngroups):\n",
    "    try:\n",
    "        conversation = \"\"\n",
    "        for _, line in df_1.iterrows():\n",
    "            if line[\"Remark\"] == \"小老正\":\n",
    "                conversation += \"老公：\" + clearn_text(line[\"StrContent\"]) + \"\\n\"\n",
    "            else:\n",
    "                conversation += \"老婆：\" + clearn_text(line[\"StrContent\"]) + \"\\n\"\n",
    "        prompt = template.format(conversation=conversation)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        results.append(response.choices[0].message.content)\n",
    "    except:\n",
    "        continue    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def clearn_text(text):\n",
    "    # 正则表达式匹配中文字符和标点符号，同时保留换行符\n",
    "    pattern = re.compile(r'[^\\u4e00-\\u9fff\\u3000-\\u303f\\uff00-\\uffef\\n]')\n",
    "\n",
    "    # 替换掉非中文字符、标点符号和换行符\n",
    "    filtered_text = re.sub(pattern, '', text)\n",
    "\n",
    "    # 去除多余的空行（保留单行换行）\n",
    "    filtered_text = re.sub(r'\\n+', '\\n', filtered_text).strip()\n",
    "    return filtered_text\n",
    "\n",
    "# 初始化 OpenAI 客户端\n",
    "client = OpenAI(\n",
    "    api_key=\"XXX\",  # 替换为你的 API Key\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    ")\n",
    "\n",
    "# 假设 df 是你的 DataFrame\n",
    "df['StrTime'] = pd.to_datetime(df['StrTime'])\n",
    "\n",
    "template = \"\"\"我正在制作我与女朋友的恋爱报告，我将给你我们某一天的对话内容。\n",
    "这一天的内容若含有非常非常甜蜜感人的句子，就将其输出为列表，若这一天的内容很平淡，就输出空列表。（注意，由于我有上千条对话内容，因此请你严格筛选，只输出恋爱中非常非常甜蜜感人的句子）\n",
    "以下是对话内容：\n",
    "```conversation\n",
    "{conversation}\n",
    "```\n",
    "最终请输出为json格式，格式示例为：\n",
    "{{\n",
    "    \"老公\": [\"甜蜜的句子1\", \"甜蜜的句子2\", ...],\n",
    "    \"老婆\": [\"甜蜜的句子1\", \"甜蜜的句子2\", ...]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def process_conversation(date, df_1):\n",
    "    try:\n",
    "        conversation = \"\"\n",
    "        for _, line in df_1.iterrows():\n",
    "            if line[\"Remark\"] == remark_1:\n",
    "                conversation += \"老公：\" + clearn_text(line[\"StrContent\"]) + \"\\n\"\n",
    "            else:\n",
    "                conversation += \"老婆：\" + clearn_text(line[\"StrContent\"]) + \"\\n\"\n",
    "        prompt = template.format(conversation=conversation)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing date {date}: {e}\")\n",
    "        return None\n",
    "\n",
    "results = []\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:  # 你可以根据需要调整 max_workers\n",
    "    futures = {executor.submit(process_conversation, date, df_1): date for date, df_1 in df.groupby(\"Date\")}\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        date = futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving result for date {date}: {e}\")\n",
    "\n",
    "# 保存结果到文件或进行其他处理\n",
    "with open(\"results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mzz_sentences = []\n",
    "for i in results:\n",
    "    try:\n",
    "        mzz_sentences += json.loads(i)[\"老公\"]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "syn_sentences = []\n",
    "for i in results:\n",
    "    try:\n",
    "        syn_sentences += json.loads(i)[\"老婆\"]\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# 初始化 OpenAI 客户端\n",
    "client = OpenAI(\n",
    "    api_key=\"XXX\",  # 替换为你的 API Key\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    ")\n",
    "\n",
    "template = \"\"\"我正在制作我与女朋友的恋爱报告，你需要帮我判断给定的文本是否是真正甜蜜感人的话。\n",
    "\n",
    "真正甜蜜的话示例：\n",
    "- 我真的！想你了！\n",
    "- 你是我向上的动力\n",
    "- 你是我心中的小太阳\n",
    "- 我真的好喜欢你呀\n",
    "\n",
    "普通对话内容示例：\n",
    "- 晚安\n",
    "- 每个孩子我都特别特别喜欢\n",
    "- 加油\n",
    "\n",
    "\n",
    "以下是句子：\n",
    "```sentence\n",
    "{sentence}\n",
    "```\n",
    "最终请输出为json格式，格式示例为：\n",
    "{{\n",
    "    \"判断\": \"是|否\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    try:\n",
    "        prompt = template.format(sentence=sentence)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        return sentence if result.get(\"判断\") == \"是\" else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sentence: {sentence}, error: {e}\")\n",
    "        return None\n",
    "\n",
    "syn_sentences_processed = []\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:  # 调整 max_workers 控制并发数\n",
    "    futures = {executor.submit(process_sentence, sentence): sentence for sentence in syn_sentences}\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        sentence = futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                syn_sentences_processed.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving result for sentence {sentence}: {e}\")\n",
    "\n",
    "print(\"处理后的句子：\", syn_sentences_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mzz_data/syn_sentences.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(syn_sentences_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 制作聊天记录卷轴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mzz_data/mzz_sentences.txt\", encoding=\"utf-8\") as f:\n",
    "    mzz_sentences = f.readlines()\n",
    "\n",
    "with open(\"mzz_data/syn_sentences.txt\", encoding=\"utf-8\") as f:\n",
    "    syn_sentences = f.readlines()\n",
    "\n",
    "all_sentences = mzz_sentences + syn_sentences\n",
    "print(f\"共{all_sentences}条句子\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调整背景图的透明度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def adjust_image_transparency(image_path, output_path, transparency=0.5):\n",
    "    \"\"\"\n",
    "    调整图片的透明度\n",
    "    :param image_path: 输入图片路径\n",
    "    :param output_path: 输出图片路径\n",
    "    :param transparency: 透明度（0-1，0 为完全透明，1 为不透明）\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGBA\")  # 转换为 RGBA 模式\n",
    "    data = image.getdata()\n",
    "\n",
    "    # 调整透明度\n",
    "    new_data = []\n",
    "    for item in data:\n",
    "        # 修改透明度（alpha 通道）\n",
    "        new_data.append((item[0], item[1], item[2], int(item[3] * transparency)))\n",
    "\n",
    "    image.putdata(new_data)\n",
    "    image.save(output_path, \"PNG\")  # 保存为 PNG 格式（支持透明度）\n",
    "\n",
    "# 示例：将背景图片透明度调整为 50%\n",
    "adjust_image_transparency(\"./mzz_data/background.png\", \"./mzz_data/background_transparent.png\", transparency=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.oxml.ns import qn\n",
    "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "\n",
    "\n",
    "# 1. 打乱列表顺序\n",
    "random.shuffle(all_sentences)\n",
    "\n",
    "# 2. 遍历列表元素，若句子长度大于15，将其均匀拆分成多个部分\n",
    "def split_long_sentences(sentences, max_length=30):\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.replace(\"\\n\", \"\")  # 去除原有换行符\n",
    "        if len(sentence) <= max_length:\n",
    "            new_sentences.append(sentence)\n",
    "        else:\n",
    "            # 计算需要拆分成几部分\n",
    "            num_parts = (len(sentence) + max_length - 1) // max_length  # 向上取整\n",
    "            part_length = (len(sentence) + num_parts - 1) // num_parts  # 均匀拆分\n",
    "            # 均匀拆分句子\n",
    "            split_parts = [sentence[i:i+part_length] for i in range(0, len(sentence), part_length)]\n",
    "            new_sentences.extend(split_parts)  # 将拆分后的部分插入列表\n",
    "    return new_sentences\n",
    "\n",
    "formatted_sentences = split_long_sentences(all_sentences)\n",
    "\n",
    "# 3. 写入 Word\n",
    "def export_to_word(sentences, filename=\"./mzz_data/卷轴内容.docx\"):\n",
    "    doc = Document()\n",
    "    \n",
    "    # 设置字体（使用支持中文的字体，如宋体）\n",
    "    style = doc.styles['Normal']\n",
    "    font = style.font\n",
    "    font.name = '宋体'\n",
    "    font.size = Pt(12)\n",
    "    font._element.rPr.rFonts.set(qn('w:eastAsia'), '宋体')  # 设置中文字体\n",
    "    \n",
    "    # 添加内容\n",
    "    for sentence in sentences:\n",
    "        paragraph = doc.add_paragraph()\n",
    "        paragraph.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER  # 居中对齐\n",
    "        paragraph.add_run(sentence).bold = True  # 加粗\n",
    "    \n",
    "    # 保存文档\n",
    "    doc.save(filename)\n",
    "    print(f\"文档已保存为 {filename}\")\n",
    "\n",
    "# 导出\n",
    "export_to_word(formatted_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键词提取及词云图绘制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def clearn_text(text):\n",
    "    # 正则表达式匹配中文字符和标点符号，同时保留换行符\n",
    "    pattern = re.compile(r'[^\\u4e00-\\u9fff\\u3000-\\u303f\\uff00-\\uffef\\n]')\n",
    "\n",
    "    # 替换掉非中文字符、标点符号和换行符\n",
    "    filtered_text = re.sub(pattern, '', text)\n",
    "\n",
    "    # 去除多余的空行（保留单行换行）\n",
    "    filtered_text = re.sub(r'\\n+', '\\n', filtered_text).strip()\n",
    "    return filtered_text\n",
    "\n",
    "# 初始化 OpenAI 客户端\n",
    "client = OpenAI(\n",
    "    api_key=\"XXX\",  # 替换为你的 API Key\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    ")\n",
    "\n",
    "# 假设 df 是你的 DataFrame\n",
    "df['StrTime'] = pd.to_datetime(df['StrTime'])\n",
    "\n",
    "template = \"\"\"我正在制作我与女朋友的恋爱报告，我想提取我们恋爱中经常提及的关键词。我将给你我们某一天的对话内容。\n",
    "这一天的内容若含有非常非常甜蜜感人的关键词，就将其输出为列表，若这一天的内容很平淡，没什么可以提取的关键词，就输出空列表。（注意，由于我有上千条对话内容，因此请你严格筛选，只输出恋爱中非常非常甜蜜感人的关键词）\n",
    "以下是对话内容：\n",
    "```conversation\n",
    "{conversation}\n",
    "```\n",
    "最终请输出为json格式，格式示例为：\n",
    "{{\n",
    "    \"关键词\": [\"关键词1\", \"关键词2\", ...],\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def process_conversation(date, df_1):\n",
    "    try:\n",
    "        conversation = \"\"\n",
    "        for _, line in df_1.iterrows():\n",
    "            if line[\"Remark\"] == remark_1:\n",
    "                conversation += \"老公：\" + clearn_text(line[\"StrContent\"]) + \"\\n\"\n",
    "            else:\n",
    "                conversation += \"老婆：\" + clearn_text(line[\"StrContent\"]) + \"\\n\"\n",
    "        prompt = template.format(conversation=conversation)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing date {date}: {e}\")\n",
    "        return None\n",
    "\n",
    "results = []\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:  # 你可以根据需要调整 max_workers\n",
    "    futures = {executor.submit(process_conversation, date, df_1): date for date, df_1 in df.groupby(\"Date\")}\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        date = futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving result for date {date}: {e}\")\n",
    "\n",
    "# 保存结果到文件或进行其他处理\n",
    "with open(\"./mzz_data/关键词.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_list = []\n",
    "for i in results:\n",
    "    try:\n",
    "        word_cloud_list.extend(json.loads(i)[\"关键词\"])\n",
    "    except:\n",
    "        pass\n",
    "word_cloud_list = pd.Series(word_cloud_list).map(lambda x: x.strip()).tolist()\n",
    "\n",
    "# 直接从word_cloud_list中删掉：你撤回了一条消息\n",
    "word_cloud_list = [i for i in word_cloud_list if i != \"你撤回了一条消息\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# 统计词频\n",
    "word_freq = Counter(word_cloud_list)\n",
    "\n",
    "# 设置词云图样式\n",
    "wordcloud = WordCloud(\n",
    "    font_path='simhei.ttf',  # 使用支持中文的字体（如黑体）\n",
    "    width=800,              # 图片宽度\n",
    "    height=400,             # 图片高度\n",
    "    background_color='white',  # 背景颜色\n",
    "    # colormap='Pastel1',     # 使用淡粉色风格的颜色主题\n",
    "    # colormap=\"inferno\",\n",
    "    prefer_horizontal=0.9,  # 水平排列的单词比例\n",
    "    max_words=100,          # 最多显示的单词数量\n",
    "    stopwords=stopwords,    # 设置停用词\n",
    "    contour_width=1,        # 轮廓宽度\n",
    "    contour_color='pink',   # 轮廓颜色\n",
    ")\n",
    "\n",
    "# 生成词云图\n",
    "wordcloud.generate_from_frequencies(word_freq)\n",
    "\n",
    "# 显示词云图\n",
    "plt.figure(figsize=(10, 5))  # 设置画布大小\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # 隐藏坐标轴\n",
    "plt.show()\n",
    "\n",
    "# 保存词云图\n",
    "wordcloud.to_file(\"./mzz_data/wordcloud_strawberry.png\")\n",
    "print(\"词云图已保存为 wordcloud_strawberry.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
