{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¾®ä¿¡æ‹çˆ±æŠ¥å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "df = pd.read_csv(\"./å¦ å¦ è€å©†ğŸª.csv\")\n",
    "df = df[[\"StrContent\", \"StrTime\", \"Remark\"]]\n",
    "df = df.astype(str)\n",
    "os.makedirs(\"./mzz_data/\", exist_ok=True)\n",
    "\n",
    "# å°†è¿™é‡Œæ›¿æ¢ä¸ºä½ ä»¬çš„å¾®ä¿¡æ˜µç§°\n",
    "remark_1 = \"å°è€æ­£\"\n",
    "remark_2 = \"å¦ å¦ è€å©†ğŸª\"\n",
    "\n",
    "# è¿™é‡Œæ›¿æ¢ä¸ºä½ ä»¬çš„å§“åç®€å†™\n",
    "name_1 = \"mzz\"\n",
    "name_2 = \"syn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‘é€è¡¨æƒ…åŒ…çš„æ¬¡æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_emoji\"] = df[\"StrContent\"].apply(lambda x: \"emoji\" in x)\n",
    "# æ€»çš„è¡¨æƒ…åŒ…å‘é€æ¬¡æ•°\n",
    "print(\"æ€»çš„è¡¨æƒ…åŒ…å‘é€æ¬¡æ•°ï¼š\" + str(df.loc[df[\"is_emoji\"]==True, \"StrContent\"].count()))\n",
    "# å°è€æ­£çš„è¡¨æƒ…åŒ…å‘é€æ¬¡æ•°\n",
    "print(f\"{remark_1}çš„è¡¨æƒ…åŒ…å‘é€æ¬¡æ•°ï¼š\" + str(df.loc[(df[\"is_emoji\"]==True) & (df[\"Remark\"]==remark_1), \"StrContent\"].count()))\n",
    "# å¦ å¦ è€å©†ğŸªçš„è¡¨æƒ…åŒ…å‘é€æ¬¡æ•°\n",
    "print(f\"{remark_2}çš„è¡¨æƒ…åŒ…å‘é€æ¬¡æ•°ï¼š\" + str(df.loc[(df[\"is_emoji\"]==True) & (df[\"Remark\"]==remark_2), \"StrContent\"].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "# ç»Ÿè®¡è¡¨æƒ…åŒ…å‘é€æ¬¡æ•°\n",
    "total_emoji = df.loc[df[\"is_emoji\"] == True, \"StrContent\"].count()\n",
    "mzz_emoji = df.loc[(df[\"is_emoji\"] == True) & (df[\"Remark\"] == remark_1), \"StrContent\"].count()\n",
    "syn_emoji = df.loc[(df[\"is_emoji\"] == True) & (df[\"Remark\"] == remark_2), \"StrContent\"].count()\n",
    "# æ•°æ®å‡†å¤‡\n",
    "labels = [name_1, name_2]  # ä½¿ç”¨ç®€å†™\n",
    "sizes = [mzz_emoji, syn_emoji]  # å„éƒ¨åˆ†çš„å¤§å°\n",
    "colors = [\"#FFB6C1\", \"#FFF5E1\"]  # ç²‰è‰²å’Œå¥¶æ²¹è‰²\n",
    "explode = (0, 0.1)  # çªå‡ºæ˜¾ç¤ºå‰ä¸¤éƒ¨åˆ†\n",
    "\n",
    "# å¯ç”¨æ‰‹ç»˜é£æ ¼\n",
    "plt.xkcd()\n",
    "\n",
    "# ç»˜åˆ¶é¥¼å›¾\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct=\"%1.1f%%\", startangle=90, shadow=True)\n",
    "\n",
    "# è®¾ç½®è‹±æ–‡æ ‡é¢˜\n",
    "plt.title(\"Pie of Emoji\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# ä¿å­˜\n",
    "plt.savefig(\"./mzz_data/è¡¨æƒ…åŒ…å‘é€æ¬¡æ•°é¥¼å›¾.png\", dpi=300)\n",
    "# æ˜¾ç¤ºå›¾è¡¨\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æœ€å¸¸å‘é€çš„è¡¨æƒ…åŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# ä¸‹è½½è¡¨æƒ…åŒ…å›¾ç‰‡çš„å‡½æ•°\n",
    "def download_emoji_image(emoji_xml, dir_path, rank, count):\n",
    "    # è§£æXML\n",
    "    root = ET.fromstring(emoji_xml)\n",
    "\n",
    "    # æ‰¾åˆ°emojiæ ‡ç­¾\n",
    "    emoji_element = root.find('.//emoji')\n",
    "    if emoji_element is None:\n",
    "        raise ValueError(\"æœªæ‰¾åˆ°emojiæ ‡ç­¾\")\n",
    "\n",
    "    # è·å–cdnurl\n",
    "    cdnurl = emoji_element.get('cdnurl')\n",
    "    if not cdnurl:\n",
    "        raise ValueError(\"cdnurlæœªæ‰¾åˆ°\")\n",
    "\n",
    "    # ä¸‹è½½å›¾ç‰‡\n",
    "    response = requests.get(cdnurl)\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}\")\n",
    "\n",
    "    # ä¿å­˜å›¾ç‰‡ï¼Œå‘½åä¸ºâ€œæ’å_å‘é€æ¬¡æ•°.jpgâ€\n",
    "    filename = os.path.join(dir_path, f\"{rank}_{count}.jpg\")\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f\"å›¾ç‰‡å·²ä¸‹è½½å¹¶ä¿å­˜ä¸º {filename}\")\n",
    "\n",
    "# åˆ›å»ºç›®å½•\n",
    "dir_path = f\"./mzz_data/emoji/{remark_2}æœ€å¸¸å‘çš„å›¾ç‰‡\"\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# è·å–æœ€å¸¸å‘çš„è¡¨æƒ…åŒ…XMLåˆ—è¡¨åŠå…¶å‘é€æ¬¡æ•°\n",
    "emoji_stats = df.loc[(df[\"is_emoji\"] == True) & (df[\"Remark\"] == remark_2), \"StrContent\"].value_counts().head(10)\n",
    "\n",
    "# éå†æ¯ä¸ªè¡¨æƒ…åŒ…XMLï¼Œä¸‹è½½å¹¶å‘½å\n",
    "for rank, (emoji_xml, count) in enumerate(emoji_stats.items()):\n",
    "    download_emoji_image(emoji_xml, dir_path, rank, count)\n",
    "\n",
    "# åˆ›å»ºç›®å½•\n",
    "dir_path = f\"./mzz_data/emoji/{remark_1}æœ€å¸¸å‘çš„å›¾ç‰‡\"\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# è·å–æœ€å¸¸å‘çš„è¡¨æƒ…åŒ…XMLåˆ—è¡¨åŠå…¶å‘é€æ¬¡æ•°\n",
    "emoji_stats = df.loc[(df[\"is_emoji\"] == True) & (df[\"Remark\"] == remark_1), \"StrContent\"].value_counts().head(10)\n",
    "\n",
    "# éå†æ¯ä¸ªè¡¨æƒ…åŒ…XMLï¼Œä¸‹è½½å¹¶å‘½å\n",
    "for rank, (emoji_xml, count) in enumerate(emoji_stats.items()):\n",
    "    download_emoji_image(emoji_xml, dir_path, rank, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "\n",
    "# å¯ç”¨ xkcd æ¨¡å¼\n",
    "plt.xkcd()\n",
    "\n",
    "# è®¾ç½®å­—ä½“ä¸ºé»‘ä½“ï¼Œè§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜\n",
    "\n",
    "\n",
    "# å°† StrTime è½¬æ¢ä¸º datetime ç±»å‹\n",
    "df[\"StrTime\"] = pd.to_datetime(df[\"StrTime\"])\n",
    "\n",
    "# æå–å°æ—¶ä¿¡æ¯\n",
    "df[\"Hour\"] = df[\"StrTime\"].dt.hour\n",
    "\n",
    "import re\n",
    "\n",
    "# å®šä¹‰å‡½æ•°ï¼Œç»Ÿè®¡æ±‰å­—é•¿åº¦\n",
    "def count_chinese_characters(text):\n",
    "    # åŒ¹é…æ‰€æœ‰ä¸­æ–‡å­—ç¬¦ï¼ˆåŒ…æ‹¬ç®€ä½“å’Œç¹ä½“ï¼‰\n",
    "    chinese_characters = re.findall(r'[\\u4e00-\\u9fff]', text)\n",
    "    return len(chinese_characters)\n",
    "\n",
    "# åº”ç”¨åˆ° DataFrame ä¸­\n",
    "df[\"WordCount\"] = df[\"StrContent\"].apply(count_chinese_characters)\n",
    "\n",
    "# æŒ‰å°æ—¶ç»Ÿè®¡èŠå¤©å­—æ•°\n",
    "hourly_word_count = df.groupby(\"Hour\")[\"WordCount\"].sum()\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# ä½¿ç”¨ plt.bar ç»˜åˆ¶æŸ±çŠ¶å›¾\n",
    "plt.bar(hourly_word_count.index, hourly_word_count.values, color=\"mediumseagreen\", edgecolor=\"black\", linewidth=2, alpha=0.7)\n",
    "\n",
    "# è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾\n",
    "plt.title(\"Word Count Per Hour\", fontsize=16)\n",
    "plt.xlabel(\"Hour\", fontsize=12)\n",
    "plt.ylabel(\"Word Count\", fontsize=12)\n",
    "\n",
    "# è®¾ç½® x è½´åˆ»åº¦\n",
    "plt.xticks(range(24), [f\"{i}:00\" for i in range(24)], rotation=45)\n",
    "\n",
    "# è°ƒæ•´å¸ƒå±€\n",
    "plt.tight_layout()\n",
    "\n",
    "# ä¿å­˜\n",
    "plt.savefig(\"./mzz_data/hourly_word_count.png\")\n",
    "# æ˜¾ç¤ºå›¾è¡¨\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# èŠå¤©æ—¥æœŸGithubçƒ­åŠ›å›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import calendar\n",
    "\n",
    "# å°† StrTime è½¬æ¢ä¸º datetime ç±»å‹\n",
    "df[\"StrTime\"] = pd.to_datetime(df[\"StrTime\"])\n",
    "\n",
    "# æå–æ—¥æœŸ\n",
    "df[\"Date\"] = df[\"StrTime\"].dt.date\n",
    "\n",
    "# ç¡®ä¿ Date åˆ—ä¸º datetime ç±»å‹\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# è¿‡æ»¤2024å¹´çš„æ•°æ®\n",
    "df_2024 = df[df['Date'].dt.year == 2024]\n",
    "\n",
    "# æŒ‰æ—¥æœŸç»Ÿè®¡èŠå¤©å­—æ•°\n",
    "daily_word_count = df_2024.groupby('Date')['WordCount'].sum().reset_index()\n",
    "\n",
    "# ä½¿ç”¨ç®±çº¿å›¾å»æ‰å¼‚å¸¸å€¼\n",
    "Q1 = daily_word_count['WordCount'].quantile(0.25)\n",
    "Q3 = daily_word_count['WordCount'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# å¼‚å¸¸å€¼å¡«è¡¥ä¸ºè¾¹ç•Œ\n",
    "daily_word_count.loc[daily_word_count['WordCount'] > upper_bound, 'WordCount'] = upper_bound\n",
    "\n",
    "# ä¸ºçƒ­åŠ›å›¾åˆ›å»ºçŸ©é˜µ\n",
    "daily_word_count['Weekday'] = daily_word_count['Date'].dt.weekday  # æ˜ŸæœŸå‡ \n",
    "daily_word_count['Week'] = daily_word_count['Date'].dt.strftime('%U')  # ç¬¬å‡ å‘¨\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªå…¨å¹´çš„ç©ºçŸ©é˜µ (52å‘¨ x 7å¤©)\n",
    "heatmap_data = pd.DataFrame(\n",
    "    index=range(7), columns=range(1, 53), data=0\n",
    ")\n",
    "\n",
    "# å¡«å……çŸ©é˜µ\n",
    "for _, row in daily_word_count.iterrows():\n",
    "    week = int(row['Week'])\n",
    "    weekday = row['Weekday']\n",
    "    heatmap_data.loc[weekday, week] = row['WordCount']\n",
    "\n",
    "\n",
    "# è·å–æ¯ä¸ªæœˆçš„èµ·å§‹æ—¥æœŸå¯¹åº”çš„å‘¨æ•°\n",
    "first_day_of_month = pd.date_range(start='2024-01-01', end='2024-12-31', freq='MS')\n",
    "month_labels = {int(date.strftime('%U')): date.strftime('%b') for date in first_day_of_month}\n",
    "\n",
    "# ä¿®æ”¹ç»˜å›¾ä»£ç \n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    cmap='Greens',\n",
    "    linewidths=0.5,\n",
    "    linecolor='white',\n",
    "    cbar_kws={'label': 'Word Count'},\n",
    "    square=True\n",
    ")\n",
    "\n",
    "\n",
    "# æ›¿æ¢æ¨ªåæ ‡çš„åˆ»åº¦æ ‡ç­¾\n",
    "weeks = range(0, 53)\n",
    "xtick_labels = [month_labels[week] if week in month_labels else '' for week in weeks]\n",
    "plt.xticks(ticks=range(0, 53), labels=xtick_labels, rotation=0)\n",
    "# è®¾ç½®æ ‡é¢˜å’Œå…¶ä»–æ ·å¼\n",
    "plt.title('Chat Activity Heatmap', fontsize=16)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Weekday')\n",
    "plt.yticks(ticks=range(7), labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'], rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./mzz_data/heatmap.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## èŠå¤©å­—æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€»èŠå¤©æ¬¡æ•°\n",
    "print(\"æ€»èŠå¤©æ¬¡æ•°ï¼š\" + str(df[\"WordCount\"].sum()))\n",
    "# å°è€æ­£çš„èŠå¤©å­—æ•°\n",
    "print(f\"{remark_1}çš„èŠå¤©å­—æ•°ï¼š\" + str(df.loc[df[\"Remark\"]==remark_1, \"WordCount\"].sum()))\n",
    "# å¦ å¦ è€å©†ğŸªçš„èŠå¤©å­—æ•°\n",
    "print(f\"{remark_2}çš„èŠå¤©å­—æ•°ï¼š\" + str(df.loc[df[\"Remark\"]==remark_2, \"WordCount\"].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "# ç»Ÿè®¡è¡¨æƒ…åŒ…å‘é€æ¬¡æ•°\n",
    "total_word = df[\"WordCount\"].sum()\n",
    "mzz_word = df.loc[df[\"Remark\"]==remark_1, \"WordCount\"].sum()\n",
    "syn_word = df.loc[df[\"Remark\"]==remark_2, \"WordCount\"].sum()\n",
    "# æ•°æ®å‡†å¤‡\n",
    "labels = [name_1, name_2]  # ä½¿ç”¨ç®€å†™\n",
    "sizes = [mzz_word, syn_word]  # å„éƒ¨åˆ†çš„å¤§å°\n",
    "colors = [\"#FFB6C1\", \"#FFF5E1\"]  # ç²‰è‰²å’Œå¥¶æ²¹è‰²\n",
    "explode = (0, 0.1)  # çªå‡ºæ˜¾ç¤ºå‰ä¸¤éƒ¨åˆ†\n",
    "\n",
    "# å¯ç”¨æ‰‹ç»˜é£æ ¼\n",
    "plt.xkcd()\n",
    "\n",
    "# ç»˜åˆ¶é¥¼å›¾\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct=\"%1.1f%%\", startangle=90, shadow=True)\n",
    "\n",
    "# è®¾ç½®è‹±æ–‡æ ‡é¢˜\n",
    "plt.title(\"Pie of word\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# ä¿å­˜\n",
    "plt.savefig(\"./mzz_data/å­—æ•°æ¬¡æ•°é¥¼å›¾.png\", dpi=300)\n",
    "# æ˜¾ç¤ºå›¾è¡¨\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## èŠå¤©æ—¶é—´æ®µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# å°† StrTime è½¬æ¢ä¸º datetime ç±»å‹\n",
    "df[\"StrTime\"] = pd.to_datetime(df[\"StrTime\"])\n",
    "\n",
    "# æå–æ—¥æœŸ\n",
    "df[\"Date\"] = df[\"StrTime\"].dt.date\n",
    "\n",
    "# ç»Ÿè®¡æ¯æ—¥æ¶ˆæ¯æ•°é‡\n",
    "daily_message_count = df.loc[df[\"is_emoji\"]==False].groupby(\"Date\").size()\n",
    "\n",
    "# çœ‹çœ‹èŠå¤©æœ€é¢‘ç¹æ˜¯å“ªå¤©\n",
    "print(\"èŠå¤©æœ€é¢‘ç¹æ˜¯å“ªå¤©ï¼š\" + str(daily_message_count.index[daily_message_count.argsort()[::-1][1]]))\n",
    "\n",
    "# çœ‹çœ‹è¿™å¤©è¯´äº†ä»€ä¹ˆ\n",
    "text = \"\\n\".join(df.loc[df[\"Date\"]==daily_message_count.index[daily_message_count.argsort()[::-1][1]]][\"StrContent\"].values)\n",
    "# æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ä¸­æ–‡å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·ï¼ŒåŒæ—¶ä¿ç•™æ¢è¡Œç¬¦\n",
    "pattern = re.compile(r'[^\\u4e00-\\u9fff\\u3000-\\u303f\\uff00-\\uffef\\n:]')\n",
    "\n",
    "# æ›¿æ¢æ‰éä¸­æ–‡å­—ç¬¦ã€æ ‡ç‚¹ç¬¦å·å’Œæ¢è¡Œç¬¦\n",
    "filtered_text = re.sub(pattern, '', text)\n",
    "\n",
    "# å»é™¤å¤šä½™çš„ç©ºè¡Œï¼ˆä¿ç•™å•è¡Œæ¢è¡Œï¼‰\n",
    "filtered_text = re.sub(r'\\n+', '\\n', filtered_text).strip()\n",
    "\n",
    "# è¿™å¤©è¯´äº†å¤šå°‘å­—\n",
    "print(\"è¿™å¤©è¯´äº†å¤šå°‘å­—ï¼š\" + str(len(filtered_text)))\n",
    "# è¿™å¤©è¯´äº†ä»€ä¹ˆ\n",
    "print(\"è¿™å¤©è¯´äº†ä»€ä¹ˆï¼š\\n\" + filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## èŠå¤©å­—æ•°éšæ—¶é—´å˜åŒ–æŠ˜çº¿å›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# å°† StrTime è½¬æ¢ä¸º datetime ç±»å‹\n",
    "df['StrTime'] = pd.to_datetime(df['StrTime'])\n",
    "\n",
    "# è¿‡æ»¤å‡ºä½ å’Œå¥³æœ‹å‹çš„èŠå¤©è®°å½•\n",
    "syn_df = df[df['Remark'] == remark_2]  # å¥³æœ‹å‹çš„èŠå¤©è®°å½•\n",
    "mzz_df = df[df['Remark'] == remark_1]      # ä½ çš„èŠå¤©è®°å½•\n",
    "\n",
    "# æŒ‰æœˆä»½ç»Ÿè®¡å­—æ•°\n",
    "syn_df['Month'] = syn_df['StrTime'].dt.to_period('M')\n",
    "mzz_df['Month'] = mzz_df['StrTime'].dt.to_period('M')\n",
    "\n",
    "syn_monthly_word_count = syn_df.groupby('Month')['WordCount'].sum().iloc[: -1]\n",
    "mzz_monthly_word_count = mzz_df.groupby('Month')['WordCount'].sum().iloc[: -1]\n",
    "\n",
    "# æ‰¾åˆ°æœ€é«˜ç‚¹çš„æœˆä»½å’Œå­—æ•°\n",
    "syn_max_month = syn_monthly_word_count.index.astype(str)[syn_monthly_word_count.argmax()]\n",
    "syn_max_word_count = syn_monthly_word_count.max()\n",
    "\n",
    "mzz_max_month = mzz_monthly_word_count.index.astype(str)[mzz_monthly_word_count.argmax()]\n",
    "mzz_max_word_count = mzz_monthly_word_count.max()\n",
    "\n",
    "# å¯ç”¨ xkcd é£æ ¼\n",
    "plt.xkcd()\n",
    "\n",
    "# ç»˜åˆ¶æŠ˜çº¿å›¾\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# ç»˜åˆ¶å¥³æœ‹å‹çš„æŠ˜çº¿å›¾\n",
    "ax.plot(syn_monthly_word_count.index.astype(str), syn_monthly_word_count.values, \n",
    "        marker='o', linestyle='-', color='lightcoral', linewidth=2, markersize=8, label=f'{name_2.upper()}\\'s Word Count')\n",
    "\n",
    "# ç»˜åˆ¶ä½ çš„æŠ˜çº¿å›¾\n",
    "ax.plot(mzz_monthly_word_count.index.astype(str), mzz_monthly_word_count.values, \n",
    "        marker='o', linestyle='-', color='mediumseagreen', linewidth=2, markersize=8, label=f'{name_1.upper()}\\'s Word Count')\n",
    "\n",
    "# æ ‡è®°å¥³æœ‹å‹çš„æœ€é«˜ç‚¹\n",
    "ax.plot(syn_max_month, syn_max_word_count, marker='*', markersize=20, color='coral', \n",
    "        markeredgecolor='black', markeredgewidth=1, label=f'{name_2.upper()}\\'s Highest Point')\n",
    "\n",
    "# æ ‡è®°ä½ çš„æœ€é«˜ç‚¹\n",
    "ax.plot(mzz_max_month, mzz_max_word_count, marker='*', markersize=20, color='gold', \n",
    "        markeredgecolor='black', markeredgewidth=1, label=f'{name_1.upper()}\\'s Highest Point')\n",
    "\n",
    "# è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾\n",
    "ax.set_title('Word Count Over Time', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Month', fontsize=12)\n",
    "ax.set_ylabel('Word Count', fontsize=12)\n",
    "\n",
    "# æ—‹è½¬ x è½´æ ‡ç­¾\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# æ˜¾ç¤ºç½‘æ ¼\n",
    "ax.grid(True, linestyle='-', alpha=0.6)\n",
    "\n",
    "# æ˜¾ç¤ºå›¾ä¾‹\n",
    "ax.legend()\n",
    "\n",
    "# è‡ªåŠ¨è°ƒæ•´å¸ƒå±€\n",
    "plt.tight_layout()\n",
    "\n",
    "# ä¿å­˜\n",
    "plt.savefig('./mzz_data/combined_word_count_over_time.png', dpi=300)\n",
    "\n",
    "# æ˜¾ç¤ºå›¾è¡¨\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2024å¹´5æœˆï¼Œæˆ‘ä»¬è¿›å…¥çƒ­æ‹æœŸï¼ŒèŠå¤©å­—æ•°è¾¾åˆ°äº†æœ€é«˜ç‚¹ï¼ˆä¸€äº›ç…§ç‰‡ï¼‰\n",
    "2024å¹´7æœˆï¼Œè€å…¬æ”¾æš‘å‡å•¦ï¼Œæˆ‘ä»¬çŸ­æš‚çš„ç»“æŸäº†å¼‚åœ°æ—¶å…‰ï¼Œç»å¸¸å‡ºå»ç©ï¼ŒèŠå¤©å­—æ•°æœ‰æ‰€ä¸‹é™\n",
    "ä¹‹åï¼Œæˆ‘ä»¬çš„èŠå¤©å­—æ•°æŒç»­ä¸Šå‡ï¼Œè¯´æ˜æˆ‘ä»¬æ„Ÿæƒ…è¶Šæ¥è¶Šå¥½ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¯¹è¯æƒ…æ„Ÿåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯\n",
    "client = OpenAI(\n",
    "    api_key=\"XXX\",  # æ›¿æ¢ä¸ºä½ çš„ API Key\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    ")\n",
    "\n",
    "\n",
    "# å°† StrTime è½¬æ¢ä¸º datetime ç±»å‹\n",
    "df['StrTime'] = pd.to_datetime(df['StrTime'])\n",
    "\n",
    "# æŒ‰å¤©åˆ†ç»„ï¼Œæ‹¼æ¥å¯¹è¯å†…å®¹\n",
    "daily_conversations = df.groupby('Date').apply(lambda x: ' '.join(x['StrContent'])).reset_index(name='Conversation')\n",
    "\n",
    "# å®šä¹‰ system_promptï¼Œå¼•å¯¼æ¨¡å‹è¾“å‡º JSON æ ¼å¼çš„æƒ…æ„Ÿåˆ†æç»“æœ\n",
    "system_prompt = \"\"\"\n",
    "ä½ æ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†æåŠ©æ‰‹ã€‚ç”¨æˆ·ä¼šæä¾›ä¸€æ®µå¯¹è¯å†…å®¹ï¼Œè¯·ä½ åˆ†æè¿™æ®µå¯¹è¯çš„æƒ…æ„Ÿå€¾å‘ï¼Œå¹¶è¾“å‡º JSON æ ¼å¼çš„ç»“æœã€‚\n",
    "\n",
    "è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š\n",
    "{\n",
    "    \"sentiment\": \"ç”œèœœ\" | \"åµæ¶\" | \"å¹³æ·¡\"\n",
    "}\n",
    "\n",
    "åˆ†æè§„åˆ™ï¼š\n",
    "1. å¦‚æœå¯¹è¯ä¸­åŒ…å«å¤§é‡ç§¯æã€æ¸©é¦¨ã€ç”œèœœçš„å†…å®¹ï¼ˆä¾‹å¦‚â€œçˆ±ä½ â€ã€â€œæƒ³ä½ â€ã€â€œå¼€å¿ƒâ€ç­‰ï¼‰ï¼Œåˆ™è¾“å‡ºâ€œç”œèœœâ€ã€‚\n",
    "2. å¦‚æœå¯¹è¯ä¸­åŒ…å«å¤§é‡è´Ÿé¢ã€äº‰åµã€ä¸æ»¡çš„å†…å®¹ï¼ˆä¾‹å¦‚â€œç”Ÿæ°”â€ã€â€œè®¨åŒâ€ã€â€œçƒ¦â€ç­‰ï¼‰ï¼Œåˆ™è¾“å‡ºâ€œåµæ¶â€ã€‚\n",
    "3. å¦‚æœå¯¹è¯å†…å®¹ä¸»è¦æ˜¯æ—¥å¸¸äº¤æµï¼Œæ²¡æœ‰æ˜æ˜¾çš„æƒ…æ„Ÿå€¾å‘ï¼Œåˆ™è¾“å‡ºâ€œå¹³æ·¡â€ã€‚\n",
    "\n",
    "è¯·æ ¹æ®å¯¹è¯å†…å®¹ï¼Œä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è§„åˆ™è¾“å‡º JSON æ ¼å¼çš„ç»“æœã€‚\n",
    "\"\"\"\n",
    "\n",
    "# å®šä¹‰é‡è¯•æœºåˆ¶\n",
    "def analyze_sentiment_with_retry(conversation, max_retries=3, delay=2):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"deepseek-chat\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": conversation}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            return json.loads(response.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            time.sleep(delay)\n",
    "    return {\"sentiment\": \"å¹³æ·¡\"}  # å¦‚æœé‡è¯•å¤šæ¬¡å¤±è´¥ï¼Œé»˜è®¤è¿”å›â€œå¹³æ·¡â€\n",
    "\n",
    "# åˆ†ææ¯ä¸€å¤©çš„æƒ…æ„Ÿå€¾å‘\n",
    "sentiment_counts = {\"ç”œèœœ\": 0, \"åµæ¶\": 0, \"å¹³æ·¡\": 0}\n",
    "\n",
    "for index, row in tqdm(daily_conversations.iterrows(), total=daily_conversations.shape[0]):\n",
    "    date = row['Date']\n",
    "    conversation = row['Conversation']\n",
    "    \n",
    "    # è°ƒç”¨ API åˆ†ææƒ…æ„Ÿ\n",
    "    result = analyze_sentiment_with_retry(conversation)\n",
    "    sentiment = result.get(\"sentiment\", \"å¹³æ·¡\")  # å¦‚æœè¿”å›ç»“æœä¸­æ²¡æœ‰ sentimentï¼Œé»˜è®¤è®¾ä¸ºâ€œå¹³æ·¡â€\n",
    "    \n",
    "    # ç»Ÿè®¡æƒ…æ„Ÿå¤©æ•°\n",
    "    sentiment_counts[sentiment] += 1\n",
    "    \n",
    "    # æ‰“å°ç»“æœ\n",
    "    print(f\"æ—¥æœŸ: {date}, æƒ…æ„Ÿ: {sentiment}\")\n",
    "\n",
    "# æ‰“å°æœ€ç»ˆç»Ÿè®¡ç»“æœ\n",
    "print(\"\\næƒ…æ„Ÿç»Ÿè®¡ç»“æœ:\")\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    print(f\"{sentiment}: {count} å¤©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# æ ‡ç­¾å’Œæ•°æ®\n",
    "labels = sentiment_counts.keys()\n",
    "sizes = sentiment_counts.values()\n",
    "\n",
    "# é¢œè‰²è®¾ç½®ï¼ˆè‰è“è›‹ç³•é£æ ¼ï¼‰\n",
    "colors = [\"#FFB6C1\", \"#A52A2A\", \"#FFF5E1\"]  # ç²‰è‰²ã€æ·±çº¢è‰²ã€å¥¶æ²¹è‰²\n",
    "explode = (0.1, 0, 0)  # çªå‡ºæ˜¾ç¤ºâ€œSweetâ€éƒ¨åˆ†\n",
    "\n",
    "# å¯ç”¨æ‰‹ç»˜é£æ ¼\n",
    "plt.xkcd()\n",
    "# è‡ªå®šä¹‰æ˜¾ç¤ºå‡½æ•°\n",
    "def autopct_format(values):\n",
    "    def my_format(pct):\n",
    "        total = sum(values)\n",
    "        val = int(round(pct * total / 100.0))\n",
    "        return f'{val}'  # æ˜¾ç¤ºåŸå§‹æ•°å­—\n",
    "    return my_format\n",
    "\n",
    "# ç»˜åˆ¶é¥¼å›¾\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct=autopct_format(sizes), startangle=90, shadow=True, wedgeprops={'edgecolor': 'black'})\n",
    "\n",
    "# è®¾ç½®è‹±æ–‡æ ‡é¢˜\n",
    "plt.title(\"Relationship Day Statistics\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# ä¿å­˜\n",
    "plt.savefig(\"./mzz_data/relationship_day_statistics.png\", dpi=300)\n",
    "\n",
    "# æ˜¾ç¤ºå›¾è¡¨\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "import re \n",
    "\n",
    "\n",
    "def clearn_text(text):\n",
    "    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ä¸­æ–‡å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·ï¼ŒåŒæ—¶ä¿ç•™æ¢è¡Œç¬¦\n",
    "    pattern = re.compile(r'[^\\u4e00-\\u9fff\\u3000-\\u303f\\uff00-\\uffef\\n]')\n",
    "\n",
    "    # æ›¿æ¢æ‰éä¸­æ–‡å­—ç¬¦ã€æ ‡ç‚¹ç¬¦å·å’Œæ¢è¡Œç¬¦\n",
    "    filtered_text = re.sub(pattern, '', text)\n",
    "\n",
    "    # å»é™¤å¤šä½™çš„ç©ºè¡Œï¼ˆä¿ç•™å•è¡Œæ¢è¡Œï¼‰\n",
    "    filtered_text = re.sub(r'\\n+', '\\n', filtered_text).strip()\n",
    "    return filtered_text\n",
    "# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯\n",
    "client = OpenAI(\n",
    "    api_key=\"XXX\",  # æ›¿æ¢ä¸ºä½ çš„ API Key\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    ")\n",
    "\n",
    "\n",
    "# å°† StrTime è½¬æ¢ä¸º datetime ç±»å‹\n",
    "df['StrTime'] = pd.to_datetime(df['StrTime'])\n",
    "\n",
    "template = \"\"\"æˆ‘æ­£åœ¨åˆ¶ä½œæˆ‘ä¸å¥³æœ‹å‹çš„æ‹çˆ±æŠ¥å‘Šï¼Œæˆ‘å°†ç»™ä½ æˆ‘ä»¬æŸä¸€å¤©çš„å¯¹è¯å†…å®¹ï¼Œè¿™ä¸€å¤©çš„å†…å®¹è‹¥å«æœ‰éå¸¸ç”œèœœéå¸¸æ„Ÿäººçš„å¥å­ï¼Œå°±å°†å…¶è¾“å‡ºä¸ºåˆ—è¡¨ï¼Œè‹¥è¿™ä¸€å¤©çš„å†…å®¹å¾ˆå¹³æ·¡ï¼Œå°±è¾“å‡ºç©ºåˆ—è¡¨ã€‚\n",
    "ä»¥ä¸‹æ˜¯å¯¹è¯å†…å®¹ï¼š\n",
    "```conversation\n",
    "{conversation}\n",
    "```\n",
    "æœ€ç»ˆè¯·è¾“å‡ºä¸ºjsonæ ¼å¼ï¼Œæ ¼å¼ç¤ºä¾‹ä¸ºï¼š\n",
    "{{\n",
    "    \"è€å…¬\": [\"ç”œèœœçš„å¥å­1\", \"ç”œèœœçš„å¥å­2\", ...],\n",
    "    \"è€å©†\": [\"ç”œèœœçš„å¥å­1\", \"ç”œèœœçš„å¥å­2\", ...]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "results = []\n",
    "for date, df_1 in tqdm(df.groupby([\"Date\"]), total=df.groupby([\"Date\"]).ngroups):\n",
    "    try:\n",
    "        conversation = \"\"\n",
    "        for _, line in df_1.iterrows():\n",
    "            if line[\"Remark\"] == \"å°è€æ­£\":\n",
    "                conversation += \"è€å…¬ï¼š\" + clearn_text(line[\"StrContent\"]) + \"\\n\"\n",
    "            else:\n",
    "                conversation += \"è€å©†ï¼š\" + clearn_text(line[\"StrContent\"]) + \"\\n\"\n",
    "        prompt = template.format(conversation=conversation)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        results.append(response.choices[0].message.content)\n",
    "    except:\n",
    "        continue    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def clearn_text(text):\n",
    "    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ä¸­æ–‡å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·ï¼ŒåŒæ—¶ä¿ç•™æ¢è¡Œç¬¦\n",
    "    pattern = re.compile(r'[^\\u4e00-\\u9fff\\u3000-\\u303f\\uff00-\\uffef\\n]')\n",
    "\n",
    "    # æ›¿æ¢æ‰éä¸­æ–‡å­—ç¬¦ã€æ ‡ç‚¹ç¬¦å·å’Œæ¢è¡Œç¬¦\n",
    "    filtered_text = re.sub(pattern, '', text)\n",
    "\n",
    "    # å»é™¤å¤šä½™çš„ç©ºè¡Œï¼ˆä¿ç•™å•è¡Œæ¢è¡Œï¼‰\n",
    "    filtered_text = re.sub(r'\\n+', '\\n', filtered_text).strip()\n",
    "    return filtered_text\n",
    "\n",
    "# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯\n",
    "client = OpenAI(\n",
    "    api_key=\"XXX\",  # æ›¿æ¢ä¸ºä½ çš„ API Key\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    ")\n",
    "\n",
    "# å‡è®¾ df æ˜¯ä½ çš„ DataFrame\n",
    "df['StrTime'] = pd.to_datetime(df['StrTime'])\n",
    "\n",
    "template = \"\"\"æˆ‘æ­£åœ¨åˆ¶ä½œæˆ‘ä¸å¥³æœ‹å‹çš„æ‹çˆ±æŠ¥å‘Šï¼Œæˆ‘å°†ç»™ä½ æˆ‘ä»¬æŸä¸€å¤©çš„å¯¹è¯å†…å®¹ã€‚\n",
    "è¿™ä¸€å¤©çš„å†…å®¹è‹¥å«æœ‰éå¸¸éå¸¸ç”œèœœæ„Ÿäººçš„å¥å­ï¼Œå°±å°†å…¶è¾“å‡ºä¸ºåˆ—è¡¨ï¼Œè‹¥è¿™ä¸€å¤©çš„å†…å®¹å¾ˆå¹³æ·¡ï¼Œå°±è¾“å‡ºç©ºåˆ—è¡¨ã€‚ï¼ˆæ³¨æ„ï¼Œç”±äºæˆ‘æœ‰ä¸Šåƒæ¡å¯¹è¯å†…å®¹ï¼Œå› æ­¤è¯·ä½ ä¸¥æ ¼ç­›é€‰ï¼Œåªè¾“å‡ºæ‹çˆ±ä¸­éå¸¸éå¸¸ç”œèœœæ„Ÿäººçš„å¥å­ï¼‰\n",
    "ä»¥ä¸‹æ˜¯å¯¹è¯å†…å®¹ï¼š\n",
    "```conversation\n",
    "{conversation}\n",
    "```\n",
    "æœ€ç»ˆè¯·è¾“å‡ºä¸ºjsonæ ¼å¼ï¼Œæ ¼å¼ç¤ºä¾‹ä¸ºï¼š\n",
    "{{\n",
    "    \"è€å…¬\": [\"ç”œèœœçš„å¥å­1\", \"ç”œèœœçš„å¥å­2\", ...],\n",
    "    \"è€å©†\": [\"ç”œèœœçš„å¥å­1\", \"ç”œèœœçš„å¥å­2\", ...]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def process_conversation(date, df_1):\n",
    "    try:\n",
    "        conversation = \"\"\n",
    "        for _, line in df_1.iterrows():\n",
    "            if line[\"Remark\"] == remark_1:\n",
    "                conversation += \"è€å…¬ï¼š\" + clearn_text(line[\"StrContent\"]) + \"\\n\"\n",
    "            else:\n",
    "                conversation += \"è€å©†ï¼š\" + clearn_text(line[\"StrContent\"]) + \"\\n\"\n",
    "        prompt = template.format(conversation=conversation)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing date {date}: {e}\")\n",
    "        return None\n",
    "\n",
    "results = []\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:  # ä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ max_workers\n",
    "    futures = {executor.submit(process_conversation, date, df_1): date for date, df_1 in df.groupby(\"Date\")}\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        date = futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving result for date {date}: {e}\")\n",
    "\n",
    "# ä¿å­˜ç»“æœåˆ°æ–‡ä»¶æˆ–è¿›è¡Œå…¶ä»–å¤„ç†\n",
    "with open(\"results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mzz_sentences = []\n",
    "for i in results:\n",
    "    try:\n",
    "        mzz_sentences += json.loads(i)[\"è€å…¬\"]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "syn_sentences = []\n",
    "for i in results:\n",
    "    try:\n",
    "        syn_sentences += json.loads(i)[\"è€å©†\"]\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯\n",
    "client = OpenAI(\n",
    "    api_key=\"XXX\",  # æ›¿æ¢ä¸ºä½ çš„ API Key\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    ")\n",
    "\n",
    "template = \"\"\"æˆ‘æ­£åœ¨åˆ¶ä½œæˆ‘ä¸å¥³æœ‹å‹çš„æ‹çˆ±æŠ¥å‘Šï¼Œä½ éœ€è¦å¸®æˆ‘åˆ¤æ–­ç»™å®šçš„æ–‡æœ¬æ˜¯å¦æ˜¯çœŸæ­£ç”œèœœæ„Ÿäººçš„è¯ã€‚\n",
    "\n",
    "çœŸæ­£ç”œèœœçš„è¯ç¤ºä¾‹ï¼š\n",
    "- æˆ‘çœŸçš„ï¼æƒ³ä½ äº†ï¼\n",
    "- ä½ æ˜¯æˆ‘å‘ä¸Šçš„åŠ¨åŠ›\n",
    "- ä½ æ˜¯æˆ‘å¿ƒä¸­çš„å°å¤ªé˜³\n",
    "- æˆ‘çœŸçš„å¥½å–œæ¬¢ä½ å‘€\n",
    "\n",
    "æ™®é€šå¯¹è¯å†…å®¹ç¤ºä¾‹ï¼š\n",
    "- æ™šå®‰\n",
    "- æ¯ä¸ªå­©å­æˆ‘éƒ½ç‰¹åˆ«ç‰¹åˆ«å–œæ¬¢\n",
    "- åŠ æ²¹\n",
    "\n",
    "\n",
    "ä»¥ä¸‹æ˜¯å¥å­ï¼š\n",
    "```sentence\n",
    "{sentence}\n",
    "```\n",
    "æœ€ç»ˆè¯·è¾“å‡ºä¸ºjsonæ ¼å¼ï¼Œæ ¼å¼ç¤ºä¾‹ä¸ºï¼š\n",
    "{{\n",
    "    \"åˆ¤æ–­\": \"æ˜¯|å¦\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    try:\n",
    "        prompt = template.format(sentence=sentence)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        return sentence if result.get(\"åˆ¤æ–­\") == \"æ˜¯\" else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sentence: {sentence}, error: {e}\")\n",
    "        return None\n",
    "\n",
    "syn_sentences_processed = []\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:  # è°ƒæ•´ max_workers æ§åˆ¶å¹¶å‘æ•°\n",
    "    futures = {executor.submit(process_sentence, sentence): sentence for sentence in syn_sentences}\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        sentence = futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                syn_sentences_processed.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving result for sentence {sentence}: {e}\")\n",
    "\n",
    "print(\"å¤„ç†åçš„å¥å­ï¼š\", syn_sentences_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mzz_data/syn_sentences.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(syn_sentences_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åˆ¶ä½œèŠå¤©è®°å½•å·è½´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mzz_data/mzz_sentences.txt\", encoding=\"utf-8\") as f:\n",
    "    mzz_sentences = f.readlines()\n",
    "\n",
    "with open(\"mzz_data/syn_sentences.txt\", encoding=\"utf-8\") as f:\n",
    "    syn_sentences = f.readlines()\n",
    "\n",
    "all_sentences = mzz_sentences + syn_sentences\n",
    "print(f\"å…±{all_sentences}æ¡å¥å­\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è°ƒæ•´èƒŒæ™¯å›¾çš„é€æ˜åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def adjust_image_transparency(image_path, output_path, transparency=0.5):\n",
    "    \"\"\"\n",
    "    è°ƒæ•´å›¾ç‰‡çš„é€æ˜åº¦\n",
    "    :param image_path: è¾“å…¥å›¾ç‰‡è·¯å¾„\n",
    "    :param output_path: è¾“å‡ºå›¾ç‰‡è·¯å¾„\n",
    "    :param transparency: é€æ˜åº¦ï¼ˆ0-1ï¼Œ0 ä¸ºå®Œå…¨é€æ˜ï¼Œ1 ä¸ºä¸é€æ˜ï¼‰\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGBA\")  # è½¬æ¢ä¸º RGBA æ¨¡å¼\n",
    "    data = image.getdata()\n",
    "\n",
    "    # è°ƒæ•´é€æ˜åº¦\n",
    "    new_data = []\n",
    "    for item in data:\n",
    "        # ä¿®æ”¹é€æ˜åº¦ï¼ˆalpha é€šé“ï¼‰\n",
    "        new_data.append((item[0], item[1], item[2], int(item[3] * transparency)))\n",
    "\n",
    "    image.putdata(new_data)\n",
    "    image.save(output_path, \"PNG\")  # ä¿å­˜ä¸º PNG æ ¼å¼ï¼ˆæ”¯æŒé€æ˜åº¦ï¼‰\n",
    "\n",
    "# ç¤ºä¾‹ï¼šå°†èƒŒæ™¯å›¾ç‰‡é€æ˜åº¦è°ƒæ•´ä¸º 50%\n",
    "adjust_image_transparency(\"./mzz_data/background.png\", \"./mzz_data/background_transparent.png\", transparency=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.oxml.ns import qn\n",
    "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "\n",
    "\n",
    "# 1. æ‰“ä¹±åˆ—è¡¨é¡ºåº\n",
    "random.shuffle(all_sentences)\n",
    "\n",
    "# 2. éå†åˆ—è¡¨å…ƒç´ ï¼Œè‹¥å¥å­é•¿åº¦å¤§äº15ï¼Œå°†å…¶å‡åŒ€æ‹†åˆ†æˆå¤šä¸ªéƒ¨åˆ†\n",
    "def split_long_sentences(sentences, max_length=30):\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.replace(\"\\n\", \"\")  # å»é™¤åŸæœ‰æ¢è¡Œç¬¦\n",
    "        if len(sentence) <= max_length:\n",
    "            new_sentences.append(sentence)\n",
    "        else:\n",
    "            # è®¡ç®—éœ€è¦æ‹†åˆ†æˆå‡ éƒ¨åˆ†\n",
    "            num_parts = (len(sentence) + max_length - 1) // max_length  # å‘ä¸Šå–æ•´\n",
    "            part_length = (len(sentence) + num_parts - 1) // num_parts  # å‡åŒ€æ‹†åˆ†\n",
    "            # å‡åŒ€æ‹†åˆ†å¥å­\n",
    "            split_parts = [sentence[i:i+part_length] for i in range(0, len(sentence), part_length)]\n",
    "            new_sentences.extend(split_parts)  # å°†æ‹†åˆ†åçš„éƒ¨åˆ†æ’å…¥åˆ—è¡¨\n",
    "    return new_sentences\n",
    "\n",
    "formatted_sentences = split_long_sentences(all_sentences)\n",
    "\n",
    "# 3. å†™å…¥ Word\n",
    "def export_to_word(sentences, filename=\"./mzz_data/å·è½´å†…å®¹.docx\"):\n",
    "    doc = Document()\n",
    "    \n",
    "    # è®¾ç½®å­—ä½“ï¼ˆä½¿ç”¨æ”¯æŒä¸­æ–‡çš„å­—ä½“ï¼Œå¦‚å®‹ä½“ï¼‰\n",
    "    style = doc.styles['Normal']\n",
    "    font = style.font\n",
    "    font.name = 'å®‹ä½“'\n",
    "    font.size = Pt(12)\n",
    "    font._element.rPr.rFonts.set(qn('w:eastAsia'), 'å®‹ä½“')  # è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "    \n",
    "    # æ·»åŠ å†…å®¹\n",
    "    for sentence in sentences:\n",
    "        paragraph = doc.add_paragraph()\n",
    "        paragraph.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER  # å±…ä¸­å¯¹é½\n",
    "        paragraph.add_run(sentence).bold = True  # åŠ ç²—\n",
    "    \n",
    "    # ä¿å­˜æ–‡æ¡£\n",
    "    doc.save(filename)\n",
    "    print(f\"æ–‡æ¡£å·²ä¿å­˜ä¸º {filename}\")\n",
    "\n",
    "# å¯¼å‡º\n",
    "export_to_word(formatted_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å…³é”®è¯æå–åŠè¯äº‘å›¾ç»˜åˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def clearn_text(text):\n",
    "    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ä¸­æ–‡å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·ï¼ŒåŒæ—¶ä¿ç•™æ¢è¡Œç¬¦\n",
    "    pattern = re.compile(r'[^\\u4e00-\\u9fff\\u3000-\\u303f\\uff00-\\uffef\\n]')\n",
    "\n",
    "    # æ›¿æ¢æ‰éä¸­æ–‡å­—ç¬¦ã€æ ‡ç‚¹ç¬¦å·å’Œæ¢è¡Œç¬¦\n",
    "    filtered_text = re.sub(pattern, '', text)\n",
    "\n",
    "    # å»é™¤å¤šä½™çš„ç©ºè¡Œï¼ˆä¿ç•™å•è¡Œæ¢è¡Œï¼‰\n",
    "    filtered_text = re.sub(r'\\n+', '\\n', filtered_text).strip()\n",
    "    return filtered_text\n",
    "\n",
    "# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯\n",
    "client = OpenAI(\n",
    "    api_key=\"XXX\",  # æ›¿æ¢ä¸ºä½ çš„ API Key\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    ")\n",
    "\n",
    "# å‡è®¾ df æ˜¯ä½ çš„ DataFrame\n",
    "df['StrTime'] = pd.to_datetime(df['StrTime'])\n",
    "\n",
    "template = \"\"\"æˆ‘æ­£åœ¨åˆ¶ä½œæˆ‘ä¸å¥³æœ‹å‹çš„æ‹çˆ±æŠ¥å‘Šï¼Œæˆ‘æƒ³æå–æˆ‘ä»¬æ‹çˆ±ä¸­ç»å¸¸æåŠçš„å…³é”®è¯ã€‚æˆ‘å°†ç»™ä½ æˆ‘ä»¬æŸä¸€å¤©çš„å¯¹è¯å†…å®¹ã€‚\n",
    "è¿™ä¸€å¤©çš„å†…å®¹è‹¥å«æœ‰éå¸¸éå¸¸ç”œèœœæ„Ÿäººçš„å…³é”®è¯ï¼Œå°±å°†å…¶è¾“å‡ºä¸ºåˆ—è¡¨ï¼Œè‹¥è¿™ä¸€å¤©çš„å†…å®¹å¾ˆå¹³æ·¡ï¼Œæ²¡ä»€ä¹ˆå¯ä»¥æå–çš„å…³é”®è¯ï¼Œå°±è¾“å‡ºç©ºåˆ—è¡¨ã€‚ï¼ˆæ³¨æ„ï¼Œç”±äºæˆ‘æœ‰ä¸Šåƒæ¡å¯¹è¯å†…å®¹ï¼Œå› æ­¤è¯·ä½ ä¸¥æ ¼ç­›é€‰ï¼Œåªè¾“å‡ºæ‹çˆ±ä¸­éå¸¸éå¸¸ç”œèœœæ„Ÿäººçš„å…³é”®è¯ï¼‰\n",
    "ä»¥ä¸‹æ˜¯å¯¹è¯å†…å®¹ï¼š\n",
    "```conversation\n",
    "{conversation}\n",
    "```\n",
    "æœ€ç»ˆè¯·è¾“å‡ºä¸ºjsonæ ¼å¼ï¼Œæ ¼å¼ç¤ºä¾‹ä¸ºï¼š\n",
    "{{\n",
    "    \"å…³é”®è¯\": [\"å…³é”®è¯1\", \"å…³é”®è¯2\", ...],\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def process_conversation(date, df_1):\n",
    "    try:\n",
    "        conversation = \"\"\n",
    "        for _, line in df_1.iterrows():\n",
    "            if line[\"Remark\"] == remark_1:\n",
    "                conversation += \"è€å…¬ï¼š\" + clearn_text(line[\"StrContent\"]) + \"\\n\"\n",
    "            else:\n",
    "                conversation += \"è€å©†ï¼š\" + clearn_text(line[\"StrContent\"]) + \"\\n\"\n",
    "        prompt = template.format(conversation=conversation)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing date {date}: {e}\")\n",
    "        return None\n",
    "\n",
    "results = []\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:  # ä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ max_workers\n",
    "    futures = {executor.submit(process_conversation, date, df_1): date for date, df_1 in df.groupby(\"Date\")}\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        date = futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving result for date {date}: {e}\")\n",
    "\n",
    "# ä¿å­˜ç»“æœåˆ°æ–‡ä»¶æˆ–è¿›è¡Œå…¶ä»–å¤„ç†\n",
    "with open(\"./mzz_data/å…³é”®è¯.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_list = []\n",
    "for i in results:\n",
    "    try:\n",
    "        word_cloud_list.extend(json.loads(i)[\"å…³é”®è¯\"])\n",
    "    except:\n",
    "        pass\n",
    "word_cloud_list = pd.Series(word_cloud_list).map(lambda x: x.strip()).tolist()\n",
    "\n",
    "# ç›´æ¥ä»word_cloud_listä¸­åˆ æ‰ï¼šä½ æ’¤å›äº†ä¸€æ¡æ¶ˆæ¯\n",
    "word_cloud_list = [i for i in word_cloud_list if i != \"ä½ æ’¤å›äº†ä¸€æ¡æ¶ˆæ¯\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# ç»Ÿè®¡è¯é¢‘\n",
    "word_freq = Counter(word_cloud_list)\n",
    "\n",
    "# è®¾ç½®è¯äº‘å›¾æ ·å¼\n",
    "wordcloud = WordCloud(\n",
    "    font_path='simhei.ttf',  # ä½¿ç”¨æ”¯æŒä¸­æ–‡çš„å­—ä½“ï¼ˆå¦‚é»‘ä½“ï¼‰\n",
    "    width=800,              # å›¾ç‰‡å®½åº¦\n",
    "    height=400,             # å›¾ç‰‡é«˜åº¦\n",
    "    background_color='white',  # èƒŒæ™¯é¢œè‰²\n",
    "    # colormap='Pastel1',     # ä½¿ç”¨æ·¡ç²‰è‰²é£æ ¼çš„é¢œè‰²ä¸»é¢˜\n",
    "    # colormap=\"inferno\",\n",
    "    prefer_horizontal=0.9,  # æ°´å¹³æ’åˆ—çš„å•è¯æ¯”ä¾‹\n",
    "    max_words=100,          # æœ€å¤šæ˜¾ç¤ºçš„å•è¯æ•°é‡\n",
    "    stopwords=stopwords,    # è®¾ç½®åœç”¨è¯\n",
    "    contour_width=1,        # è½®å»“å®½åº¦\n",
    "    contour_color='pink',   # è½®å»“é¢œè‰²\n",
    ")\n",
    "\n",
    "# ç”Ÿæˆè¯äº‘å›¾\n",
    "wordcloud.generate_from_frequencies(word_freq)\n",
    "\n",
    "# æ˜¾ç¤ºè¯äº‘å›¾\n",
    "plt.figure(figsize=(10, 5))  # è®¾ç½®ç”»å¸ƒå¤§å°\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # éšè—åæ ‡è½´\n",
    "plt.show()\n",
    "\n",
    "# ä¿å­˜è¯äº‘å›¾\n",
    "wordcloud.to_file(\"./mzz_data/wordcloud_strawberry.png\")\n",
    "print(\"è¯äº‘å›¾å·²ä¿å­˜ä¸º wordcloud_strawberry.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
